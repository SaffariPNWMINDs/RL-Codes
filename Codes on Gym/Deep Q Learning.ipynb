{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Policy network\n",
    "class QNet(nn.Module):\n",
    "    # Policy Network\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.1, eps_decay=0.995, target=200, chkpt=False):\n",
    "    score_hist = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    \n",
    "    for idx_epi in pbar:\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        \n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            \n",
    "            # FIX: Unpack all 5 values from gymnasium\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated  # Combine done flags\n",
    "            \n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        score_avg = np.mean(score_hist[-100:])\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        # Early stop\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    return score_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            \n",
    "            # FIX: Unpack all 5 values from gymnasium\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScore(scores):\n",
    "    plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 100\n",
    "TARGET_SCORE = 250.     # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 5          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")  \n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    ")\n",
    "\n",
    "score_hist = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=SAVE_CHKPT)\n",
    "plotScore(score_hist)\n",
    "\n",
    "if str(device) == \"cuda\":\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLander(env, agent, loop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def TextOnImg(img, score):\n",
    "    img = Image.fromarray(img)\n",
    "    font = ImageFont.truetype('/Library/Fonts/arial.ttf', 18)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((20, 20), f\"Score={score: .2f}\", font=font, fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(img)\n",
    "\n",
    "def save_frames_as_gif(frames, filename, path=\"gifs/\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print(\"Saving gif...\", end=\"\")\n",
    "    imageio.mimsave(path + filename + \".gif\", frames, fps=60)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "def gym2gif(env, agent, filename=\"gym_animation\", loop=3):\n",
    "    frames = []\n",
    "    \n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()  # FIX 1: Unpack tuple\n",
    "        score = 0\n",
    "        \n",
    "        for idx_step in range(500):\n",
    "            frame = env.render()  # FIX 2: Remove mode parameter\n",
    "            frames.append(TextOnImg(frame, score))\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            \n",
    "            # FIX 3: Unpack all 5 values\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    env.close()\n",
    "    save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "gym2gif(env, agent, loop=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
